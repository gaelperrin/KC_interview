{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.datasets as datasets\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import save\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_csv = 'dataKCx.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxNormalizer:\n",
    "    def __init__(self):\n",
    "        self.min_val = None\n",
    "        self.max_val = None\n",
    "\n",
    "    def fit(self, tensor):\n",
    "        self.min_val = tensor.min().values[0]\n",
    "        self.max_val = tensor.max().values[0]\n",
    "\n",
    "    def normalize(self, tensor):\n",
    "        if self.min_val is None or self.max_val is None:\n",
    "            raise ValueError(\"Call 'fit' first to compute min-max values.\")\n",
    "        return (tensor - self.min_val) / (self.max_val - self.min_val)\n",
    "\n",
    "    def fit_normalize(self, tensor):\n",
    "        self.fit(tensor)\n",
    "        return self.normalize(tensor)\n",
    "\n",
    "    def denormalize(self, normalized_tensor):\n",
    "        if self.min_val is None or self.max_val is None:\n",
    "            raise ValueError(\"Call 'fit' first to compute min-max values.\")\n",
    "        return normalized_tensor * (self.max_val - self.min_val) + self.min_val\n",
    "\n",
    "class VolumeDataset(Dataset):\n",
    "    def __init__(self, path_to_csv, Sn=104*3 + 1, dataset='train', model_name = 'LSTM'):\n",
    "        '''\n",
    "        Sn: how many days back to look for the continuous or opening volumes\n",
    "        dataset: train, validation or test\n",
    "        '''\n",
    "\n",
    "        self.data = pd.read_csv(path_to_csv)\n",
    "        self.Sn = Sn\n",
    "        #self.train = train\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.data.rename(columns = {\"Unnamed: 0\":\"id\"}, inplace=True)\n",
    "\n",
    "        #add dummy timesamp for opening and closing auction\n",
    "        opening_time = pd.Timestamp('06:55:00').strftime(\"%H:%M:%S\")\n",
    "        self.data.loc[self.data['tradingphase'] == 'OPENING AUCTION','time'] = opening_time\n",
    "        closing_time = pd.Timestamp('15:30:00').strftime(\"%H:%M:%S\")\n",
    "        self.data.loc[self.data['tradingphase'] == 'CLOSING AUCTION','time'] = closing_time\n",
    "        self.data['date_time'] = pd.to_datetime(self.data['date'] + 'T' + self.data['time'])\n",
    "        self.data.set_index('date_time')\n",
    "\n",
    "        #period k and date d integers used in final performance evaluation\n",
    "        self.data['k'] = (self.data['id']-1)%104 + 1\n",
    "        self.data['d'] = (self.data['id'] -1)//104 + 1\n",
    "        #pd.get_dummies(self.data.dw).rename({'0':'dw'})\n",
    "\n",
    "        #compute total volume\n",
    "        self.total_volume_per_day = self.data.groupby('date')['volume'].sum()\n",
    "        self.data = self.data.merge(self.total_volume_per_day, on='date', how='left')\n",
    "        self.data.rename(columns = {\"volume_x\":\"volume\", \"volume_y\":\"total_volume\"}, inplace=True)\n",
    "\n",
    "        ##compute total volume norm\n",
    "        #self.total_volume_per_day_norm = self.total_volume_per_day/168000000\n",
    "        #self.data = self.data.merge(self.total_volume_per_day_norm, on='date', how='left')\n",
    "        #self.data.rename(columns = {\"volume_x\":\"volume\", \"volume_y\":\"total_volume_norm\"}, inplace=True)\n",
    "\n",
    "        #compute cumulated sum\n",
    "        self.data['cum_volume']  = self.data['volume'].cumsum()\n",
    "\n",
    "        #compute missing intraday\n",
    "        self.data['missing_intraday'] = 104 - self.data['k']\n",
    "        #compute week 1-hot\n",
    "        self.data['m'] = self.data['date_time'].dt.month\n",
    "        self.data['dw'] = self.data['date_time'].dt.dayofweek\n",
    "        self.data = pd.concat([self.data, pd.get_dummies(self.data['m'])], axis=1)\n",
    "        self.data.rename(columns = {1:\"m1\", 2:\"m2\", 3:\"m3\", 4:\"m4\", 5:\"m5\", 6:\"m6\", 7:\"m7\", 8:\"m8\", 9:\"m9\", 10:\"m10\", 11:\"m11\", 12:\"m12\"}, inplace=True)\n",
    "        self.data = pd.concat([self.data, pd.get_dummies(self.data['dw'])], axis=1)\n",
    "        self.data.rename(columns = {0:\"dw0\", 1:\"dw1\", 2:\"dw2\", 3:\"dw3\", 4:\"dw4\"}, inplace=True)\n",
    "\n",
    "\n",
    "        #separated data in train, validation, test\n",
    "        #use about 20% for validation\n",
    "        end_train_day = 348 #2021-10-08\n",
    "        end_valid_day = 438 #2022-03-08\n",
    "        self.train_dataset = self.data[self.data['d'] <= end_train_day]\n",
    "        self.validation_dataset = self.data[(self.data['d'] > end_train_day) & (self.data['d'] <= end_valid_day)]\n",
    "        self.test_dataset = self.data[(self.data['d'] > end_valid_day)]\n",
    "        #last_training_date_dt = dt.datetime.strptime(last_training_date, '%Y-%m-%d').date()\n",
    "        #self.train_dataset = self.data[self.data['date_time'].dt.date <= last_training_date_dt]\n",
    "        #self.test_dataset = self.data[self.data['date_time'].dt.date > last_training_date_dt]\n",
    "        #self.test_dataset = self.data[self.data['date'] > last_training_date]\n",
    "        #self.train_dataset = self.data[self.data['date'] <= last_training_date]\n",
    "        #self.test_dataset = self.data[self.data['date'] > last_training_date]\n",
    "\n",
    "        ####\n",
    "        ##Normalization\n",
    "        ####\n",
    "        self.scaler= MinMaxScaler(feature_range=(0, 1))\n",
    "        self.train_dataset.loc[:, 'volume'] = self.scaler.fit_transform(self.train_dataset[['volume']])\n",
    "        self.validation_dataset.loc[:, 'volume'] = self.scaler.transform(self.validation_dataset[['volume']])\n",
    "        self.test_dataset.loc[:, 'volume'] = self.scaler.transform(self.test_dataset[['volume']])\n",
    "\n",
    "        self.total_volume_normalizer = MinMaxNormalizer()\n",
    "        self.train_dataset.loc[:, 'total_volume'] = self.total_volume_normalizer.fit_normalize(self.train_dataset[['total_volume']])\n",
    "        self.validation_dataset.loc[:, 'total_volume'] = self.total_volume_normalizer.normalize(self.validation_dataset[['total_volume']])\n",
    "        self.test_dataset.loc[:, 'total_volume'] = self.total_volume_normalizer.normalize(self.test_dataset[['total_volume']])\n",
    "\n",
    "        self.cum_volume_normalizer = MinMaxNormalizer()\n",
    "        self.train_dataset.loc[:, 'cum_volume'] = self.cum_volume_normalizer.fit_normalize(self.train_dataset[['cum_volume']])\n",
    "        self.validation_dataset.loc[:, 'cum_volume'] = self.cum_volume_normalizer.normalize(self.validation_dataset[['cum_volume']])\n",
    "        self.test_dataset.loc[:, 'cum_volume'] = self.cum_volume_normalizer.normalize(self.test_dataset[['cum_volume']])\n",
    "\n",
    "    def __len__(self):\n",
    "        assert self.dataset in ['train', 'validation', 'test'], 'Wrong dataset parameter'\n",
    "        if self.dataset == 'train':\n",
    "            return len(self.train_dataset) - self.Sn + 1\n",
    "        elif self.dataset == 'validation':\n",
    "            return len(self.validation_dataset) - self.Sn + 1\n",
    "        elif self.dataset == 'test':\n",
    "            return len(self.test_dataset) - self.Sn + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset == 'train':\n",
    "            output_dataset = self.train_dataset[idx:idx + self.Sn]\n",
    "        elif self.dataset == 'validation':\n",
    "            output_dataset = self.validation_dataset[idx:idx + self.Sn]\n",
    "        elif self.dataset == 'test':\n",
    "            output_dataset = self.test_dataset[idx:idx + self.Sn]\n",
    "\n",
    "        Sn = self.Sn\n",
    "\n",
    "        if self.model_name == 'FNN' or self.model_name == 'LSTM':\n",
    "            volume = output_dataset['volume']\n",
    "            total_volume = output_dataset['total_volume']\n",
    "            missing_intraday = output_dataset['missing_intraday']\n",
    "            return {\n",
    "                'volume' : torch.tensor(volume.values, dtype=torch.float32),\n",
    "                'total_volume' : torch.tensor(total_volume.values, dtype=torch.float32),\n",
    "                'missing_intraday' : torch.tensor(missing_intraday.values, dtype=torch.float32),\n",
    "            }\n",
    "        elif self.model_name == 'LSTM2':\n",
    "            volume = output_dataset['volume']\n",
    "            total_volume = output_dataset['total_volume']\n",
    "            missing_intraday = output_dataset['missing_intraday']\n",
    "            return {\n",
    "                'volume' : torch.tensor(volume.values, dtype=torch.float32),\n",
    "                'total_volume' : torch.tensor(total_volume.values, dtype=torch.float32),\n",
    "                'missing_intraday' : torch.tensor(missing_intraday.values, dtype=torch.float32),\n",
    "            }\n",
    "\n",
    "        elif self.model_name == 'LSTM4input':\n",
    "            missing_intraday = 104 - (idx + 1)%104\n",
    "            output_dataset.insert(0, 'periods_to_end', range(Sn + missing_intraday, missing_intraday, -1))\n",
    "            periods_to_end = output_dataset['periods_to_end']/(104*Sn)\n",
    "            volume = output_dataset['volume']\n",
    "            total_volume = output_dataset['total_volume']\n",
    "            week_day = output_dataset['dw']\n",
    "            month = output_dataset['m']\n",
    "            return {\n",
    "                'volume' : torch.tensor(volume.values, dtype=torch.float32),\n",
    "                'total_volume' : torch.tensor(total_volume.values, dtype=torch.float32),\n",
    "                'periods_to_end' : torch.tensor(periods_to_end.values, dtype=torch.float32),\n",
    "                'week_day' : torch.tensor(week_day.values, dtype=torch.float32),\n",
    "                'month' : torch.tensor(month.values, dtype=torch.float32)\n",
    "            }\n",
    "        elif self.model_name == \"LSTM_cumVolume_periodsToEnd\":\n",
    "            missing_intraday = 104 - (idx + 1)%104\n",
    "            output_dataset.insert(0, 'periods_to_end', range(Sn + missing_intraday, missing_intraday, -1))\n",
    "            periods_to_end = output_dataset['periods_to_end']/(104*Sn)\n",
    "            total_volume = output_dataset['total_volume']\n",
    "            cum_volume = output_dataset['cum_volume']\n",
    "            return {\n",
    "                'cum_volume' : torch.tensor(cum_volume.values, dtype=torch.float32),\n",
    "                'total_volume' : torch.tensor(total_volume.values, dtype=torch.float32),\n",
    "                'periods_to_end' : torch.tensor(periods_to_end.values, dtype=torch.float32),\n",
    "            }\n",
    "\n",
    "        elif self.model_name == 'EXPERIMENT':\n",
    "            #add periods to end\n",
    "            missing_intraday = 104 - (idx + 1)%104\n",
    "            output_dataset.insert(0, 'periods_to_end', range(Sn + missing_intraday, missing_intraday, -1))\n",
    "            #CONTINUOUS\n",
    "            #output_dataset_close = output_dataset[output_dataset['k'] == 104]\n",
    "            #output_dataset = output_dataset[output_dataset['k'] != 104]\n",
    "            periods_to_end = output_dataset['periods_to_end']/(104*Sn)\n",
    "            volume = output_dataset['volume']\n",
    "            total_volume = output_dataset['total_volume']\n",
    "            week_day = output_dataset['dw']\n",
    "            month = output_dataset['m']\n",
    "            #CLOSING DATA\n",
    "            #volume_close = output_dataset_close['volume']\n",
    "            return {\n",
    "                'volume' : torch.tensor(volume.values, dtype=torch.float32),\n",
    "                'total_volume' : torch.tensor(total_volume.values, dtype=torch.float32),\n",
    "                'periods_to_end' : torch.tensor(periods_to_end.values, dtype=torch.float32),\n",
    "                'week_day' : torch.tensor(week_day.values, dtype=torch.float32),\n",
    "                'dw_one_hot': torch.tensor(output_dataset[['dw0','dw1','dw2','dw3','dw4']].values, dtype=torch.float32),\n",
    "                'm_one_hot': torch.tensor(output_dataset[['m1','m2','m3','m4','m5','m6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12']].values, dtype=torch.float32),\n",
    "                #'volume_close':torch.tensor(volume_close.values, dtype=torch.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_model(model=None,\n",
    "                         input_path='dataKCx.csv',\n",
    "                         model_name='my_model',\n",
    "                         Sn = 3*104 + 1,\n",
    "                         optimizer=None,\n",
    "                         batch_size=50,\n",
    "                         num_epochs=10,\n",
    "                         version=0,\n",
    "                         save_plots=False,\n",
    "                         save_model=False,\n",
    "                         num_workers=1):\n",
    "    #define loss and performance metric\n",
    "    class RMSE(nn.Module):\n",
    "        def forward(self, y_pred, y_true):\n",
    "            squared_error = (y_pred - y_true) ** 2\n",
    "            ratio = squared_error/y_true**2\n",
    "            return torch.sqrt(ratio.mean())\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    rmse = RMSE()\n",
    "\n",
    "    #get datasets\n",
    "    train = VolumeDataset(path_to_csv, Sn=Sn, dataset='train', model_name=model_name)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    test = VolumeDataset(path_to_csv, Sn=Sn, dataset='validation', model_name=model_name)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    denorm = train.total_volume_normalizer.denormalize\n",
    "\n",
    "    ###\n",
    "    #TRAINING\n",
    "    ###\n",
    "\n",
    "    def train(data_loader, model, loss_fn, optimizer):\n",
    "        model.train()\n",
    "        loss_list = []\n",
    "        rmse_loss_list = []\n",
    "        #import time\n",
    "        #start = time.time()\n",
    "        for batch in data_loader:\n",
    "            prediction = model(model.prepare_input(batch))\n",
    "            target = model.prepare_target(batch)\n",
    "            loss = loss_fn(prediction, target)\n",
    "            loss_list.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                rmse_loss = rmse(denorm(prediction), denorm(target))\n",
    "                rmse_loss_list.append(rmse_loss.item())\n",
    "        average_loss = sum(loss_list)/len(loss_list)\n",
    "        average_rmse_loss = sum(rmse_loss_list)/len(rmse_loss_list)\n",
    "        return average_loss, average_rmse_loss\n",
    "\n",
    "    def test(data_loader, model, loss_fn):\n",
    "        model.eval()\n",
    "        loss_list = []\n",
    "        rmse_loss_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                prediction = model(model.prepare_input(batch))\n",
    "                target = model.prepare_target(batch)\n",
    "                loss = loss_fn(prediction, target)\n",
    "                loss_list.append(loss.item())\n",
    "                rmse_loss = rmse(denorm(prediction), denorm(target))\n",
    "                rmse_loss_list.append(rmse_loss.item())\n",
    "            average_loss = sum(loss_list)/len(loss_list)\n",
    "            average_rmse_loss = sum(rmse_loss_list)/len(rmse_loss_list)\n",
    "            return average_loss, average_rmse_loss\n",
    "\n",
    "    loss_list_train = []\n",
    "    rmse_loss_list_train = []\n",
    "    loss_list_test = []\n",
    "    rmse_loss_list_test = []\n",
    "    for epoch in range(num_epochs):\n",
    "        average_loss_train, average_rmse_loss_train = train(train_loader, model, criterion, optimizer)\n",
    "        average_loss_test, average_rmse_loss_test = test(test_loader, model, criterion)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_loss_train}, Validation Loss: {average_loss_test}, RMSE Train Loss: {average_rmse_loss_train}, RMSE Validation Loss: {average_rmse_loss_test}\")\n",
    "        loss_list_train.append(average_loss_train)\n",
    "        rmse_loss_list_train.append(average_rmse_loss_train)\n",
    "        loss_list_test.append(average_loss_test)\n",
    "        rmse_loss_list_test.append(average_rmse_loss_test)\n",
    "\n",
    "    #do plot\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.figure(0)\n",
    "    plt.plot(epochs, loss_list_train, label='Training Loss', color='b')\n",
    "    plt.plot(epochs, loss_list_test, label='Validation Loss', color='r')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Training Loss vs. Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    if save_plots:\n",
    "        working_directory = os.path.dirname(os.path.realpath(__file__))\n",
    "        plt.savefig(f'MSE_loss_{model_name}_{version}.png')\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(epochs, rmse_loss_list_train, label='Training Loss', color='b')\n",
    "    plt.plot(epochs, rmse_loss_list_test, label='Validation Loss', color='r')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('RMSE Loss')\n",
    "    plt.title('Training Loss vs. Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    if save_plots:\n",
    "        plt.savefig(f'{working_directory}/MSE_loss_{model_name}_{version}.png')\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict, f'{working_directory}/{model_name}_{version}.pth')\n",
    "\n",
    "    ###\n",
    "    #Performances\n",
    "    ###\n",
    "\n",
    "    train = VolumeDataset(path_to_csv, Sn=Sn, dataset='train', model_name=model_name)\n",
    "    train_loader_eval = DataLoader(train, batch_size=104, shuffle=False)\n",
    "    test = VolumeDataset(path_to_csv, Sn=Sn, dataset='test', model_name=model_name)\n",
    "    test_loader_eval = DataLoader(test, batch_size=104, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    loss_list_train = []\n",
    "    rmse_loss_list_train = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader_eval:\n",
    "            prediction = model(model.prepare_input(batch))\n",
    "            target = model.prepare_target(batch)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss_list_train.append(loss.item())\n",
    "            rmse_loss = rmse(denorm(prediction), denorm(target))\n",
    "            rmse_loss_list_train.append(rmse_loss.item())\n",
    "    loss_list_test = []\n",
    "    rmse_loss_list_test = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader_eval:\n",
    "            prediction = model(model.prepare_input(batch))\n",
    "            target = model.prepare_target(batch)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss_list_test.append(loss.item())\n",
    "            rmse_loss = rmse(denorm(prediction), denorm(target))\n",
    "            rmse_loss_list_test.append(rmse_loss.item())\n",
    "\n",
    "    plt.plot(3)\n",
    "    plt.plot(range(1, len(loss_list_train) + 1), loss_list_train)\n",
    "    plt.plot(range(len(loss_list_train) + 1, len(loss_list_train) + len(loss_list_test) + 1), loss_list_test)\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([0, 2])\n",
    "    plt.title('Train and test MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    if save_plots:\n",
    "        plt.savefig(f'{working_directory}/MSE_loss_days_{model_name}_{version}.png')\n",
    "\n",
    "    plt.plot(4)\n",
    "    plt.plot(range(1, len(rmse_loss_list_train) + 1), rmse_loss_list_train)\n",
    "    plt.plot(range(len(rmse_loss_list_train) + 1, len(rmse_loss_list_train) + len(rmse_loss_list_test) + 1), rmse_loss_list_test)\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('RMSE Loss')\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.title('Train and test RMSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    if save_plots:\n",
    "        plt.savefig(f'{working_directory}/RMSE_loss_days_{model_name}_{version}.png')\n",
    "\n",
    "    print(f'MSE train {sum(loss_list_train)/len(loss_list_train)}, MSE test {sum(loss_list_test)/len(loss_list_test)}')\n",
    "    print(f'RMSE train {sum(rmse_loss_list_train)/len(rmse_loss_list_train)}, RMSE test {sum(rmse_loss_list_test)/len(rmse_loss_list_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_cumVolume_periodsToEnd(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LSTM_cumVolume_periodsToEnd, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, 64, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)  # Output layer (single neuron for regression)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def prepare_input(self, batch):\n",
    "        #batch, Sn,\n",
    "        return torch.stack((batch['cum_volume'].to(device), batch['periods_to_end'].to(device)), axis=2)\n",
    "\n",
    "    def prepare_target(self, batch):\n",
    "        return batch['total_volume'][:,-1:].to(device)\n",
    "\n",
    "#LSTM2\n",
    "\n",
    "past_input_n_days = 3\n",
    "Sn = past_input_n_days*104 + 1 #past record + current record\n",
    "input_size = 2 #cum_volume and periods to end\n",
    "model_name = 'LSTM_cumVolume_periodsToEnd'\n",
    "model = LSTM_cumVolume_periodsToEnd(input_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "batch_size=50\n",
    "\n",
    "train_and_eval_model(model=model,\n",
    "                     input_path=path_to_csv,\n",
    "                     model_name=\"LSTM_cumVolume_periodsToEnd\",\n",
    "                     Sn = Sn,\n",
    "                     optimizer=optimizer,\n",
    "                     batch_size=batch_size,\n",
    "                     save_plots=False,\n",
    "                     save_model=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KP_tech_interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
